{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JmnQIm5XEKue",
        "outputId": "123c180c-958f-4a24-b061-5529e41be7eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 56ms/step - accuracy: nan - loss: nan\n",
            "Epoch 2/20\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - accuracy: nan - loss: nan\n",
            "Epoch 3/20\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - accuracy: nan - loss: nan\n",
            "Epoch 4/20\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - accuracy: nan - loss: nan\n",
            "Epoch 5/20\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - accuracy: nan - loss: nan\n",
            "Epoch 6/20\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - accuracy: nan - loss: nan\n",
            "Epoch 7/20\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - accuracy: nan - loss: nan\n",
            "Epoch 8/20\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - accuracy: nan - loss: nan\n",
            "Epoch 9/20\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - accuracy: nan - loss: nan\n",
            "Epoch 10/20\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - accuracy: nan - loss: nan\n",
            "Epoch 11/20\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - accuracy: nan - loss: nan\n",
            "Epoch 12/20\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - accuracy: nan - loss: nan\n",
            "Epoch 13/20\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - accuracy: nan - loss: nan\n",
            "Epoch 14/20\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 22ms/step - accuracy: nan - loss: nan\n",
            "Epoch 15/20\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - accuracy: nan - loss: nan\n",
            "Epoch 16/20\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 23ms/step - accuracy: nan - loss: nan\n",
            "Epoch 17/20\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 27ms/step - accuracy: nan - loss: nan\n",
            "Epoch 18/20\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - accuracy: nan - loss: nan\n",
            "Epoch 19/20\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - accuracy: nan - loss: nan\n",
            "Epoch 20/20\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 23ms/step - accuracy: nan - loss: nan\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "# Connect to TPU\n",
        "resolver = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "tf.config.experimental_connect_to_cluster(resolver)\n",
        "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "strategy = tf.distribute.TPUStrategy(resolver)\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"spoc_cleaned_final.csv\")\n",
        "\n",
        "# Tokenization\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "MAX_VOCAB_SIZE = 10000\n",
        "MAX_SEQ_LENGTH = 150\n",
        "\n",
        "# Initialize tokenizers\n",
        "tokenizer_pseudo = Tokenizer(num_words=MAX_VOCAB_SIZE, filters='', oov_token=\"<OOV>\")\n",
        "tokenizer_cpp = Tokenizer(num_words=MAX_VOCAB_SIZE, filters='', oov_token=\"<OOV>\")\n",
        "\n",
        "# Fit tokenizers\n",
        "tokenizer_pseudo.fit_on_texts(df[\"pseudocode\"].astype(str))\n",
        "tokenizer_cpp.fit_on_texts(df[\"cpp_code_cleaned\"].astype(str))\n",
        "\n",
        "# Convert to sequences\n",
        "pseudo_seq = tokenizer_pseudo.texts_to_sequences(df[\"pseudocode\"].astype(str))\n",
        "cpp_seq = tokenizer_cpp.texts_to_sequences(df[\"cpp_code_cleaned\"].astype(str))\n",
        "\n",
        "# Pad sequences\n",
        "pseudo_seq = pad_sequences(pseudo_seq, maxlen=MAX_SEQ_LENGTH, padding=\"post\", truncating=\"post\")\n",
        "cpp_seq = pad_sequences(cpp_seq, maxlen=MAX_SEQ_LENGTH, padding=\"post\", truncating=\"post\")\n",
        "\n",
        "# Save Tokenizers\n",
        "with open(\"tokenizer_pseudo.pkl\", \"wb\") as f:\n",
        "    pickle.dump(tokenizer_pseudo, f)\n",
        "\n",
        "with open(\"tokenizer_cpp.pkl\", \"wb\") as f:\n",
        "    pickle.dump(tokenizer_cpp, f)\n",
        "\n",
        "# Transformer Model\n",
        "class Transformer(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, seq_length, embed_dim=256, num_heads=8, ff_dim=512):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embed_dim, mask_zero=True)\n",
        "        self.pos_encoding = tf.keras.layers.Embedding(seq_length, embed_dim)\n",
        "        self.encoder = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(ff_dim, activation='relu'),\n",
        "            tf.keras.layers.Dense(embed_dim)\n",
        "        ])\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.output_layer = tf.keras.layers.Dense(vocab_size)  # No activation (from_logits=True in loss)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.embedding(inputs) + self.pos_encoding(tf.range(inputs.shape[1]))\n",
        "        attn_output = self.encoder(x, x)\n",
        "        out1 = self.layernorm1(x + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        out2 = self.layernorm2(out1 + ffn_output)\n",
        "        return self.output_layer(out2)\n",
        "\n",
        "# Strategy for TPU training\n",
        "with strategy.scope():\n",
        "    transformer_model = Transformer(MAX_VOCAB_SIZE, MAX_SEQ_LENGTH)\n",
        "\n",
        "    # Using Adam with a lower learning rate for stability\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
        "\n",
        "    # Using CategoricalCrossentropy with from_logits=True\n",
        "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "    transformer_model.compile(optimizer=optimizer, loss=loss_fn, metrics=[\"accuracy\"])\n",
        "\n",
        "# Shift labels for training\n",
        "pseudo_seq_shifted = np.roll(cpp_seq, shift=-1, axis=1)\n",
        "\n",
        "# Training\n",
        "transformer_model.fit(pseudo_seq, pseudo_seq_shifted, batch_size=64, epochs=20)\n",
        "\n",
        "# ✅ Save Model (Corrected)\n",
        "transformer_model.save(\"transformer_pseudo_cpp.keras\")  # Corrected save format\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"spoc_cleaned_final.csv\")\n",
        "print(df.isnull().sum())  # Check for missing values\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZ-IJ-F8CjsP",
        "outputId": "e15314f0-56db-4de1-b65f-7de51a0aa688"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pseudocode          0\n",
            "cpp_code_cleaned    0\n",
            "cpp_errors          0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_cpp = \"int main() { return 0; }\"\n",
        "tokenized_cpp = tokenizer_cpp.texts_to_sequences([example_cpp])\n",
        "print(\"Tokenized C++:\", tokenized_cpp)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZCxt-sbYDNGm",
        "outputId": "97376d6b-b793-4ee3-b776-8b8a9a5e3892"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized C++: [[40, 18, 32, 10, 11, 1994]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_cpp = \"int main() { return 0; }\"\n",
        "tokenized_cpp = tokenizer_cpp.texts_to_sequences([example_cpp])\n",
        "print(\"Tokenized C++:\", tokenized_cpp)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xe69_O5XDQ8B",
        "outputId": "c033cfe6-fe1d-4158-81fc-ed5519594ab9"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized C++: [[40, 18, 32, 10, 11, 1994]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Pseudocode Vocabulary Size:\", len(tokenizer_pseudo_to_cpp.word_index))\n",
        "print(\"C++ Vocabulary Size:\", len(tokenizer_cpp.word_index))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gZYodpWVDUqs",
        "outputId": "d0311ca4-8b84-4919-9dd2-140fc83a1575"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pseudocode Vocabulary Size: 26325\n",
            "C++ Vocabulary Size: 57604\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf transformer_pseudo_cpp.keras tokenizer_cpp.pkl tokenizer_pseudo_to_cpp.pkl\n"
      ],
      "metadata": {
        "id": "d_GmA5kxDY3d"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_cpp(\"INPUT X\", tokenizer_pseudo_to_cpp, tokenizer_cpp, model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "FcSenQK-DdTm",
        "outputId": "52ce4d35-324d-455e-8586-7aab3cdc59d5"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1][i] {\\ntemp.clear();\\nfor \"?\" b.size() \"june\";\\nm[6] {\\na[x] a[i]) nr vec(n str[i];\\nfor endl;\\nelse x2);\\ncout \"|o.o.o.o.#.#.#.#.#.#.#.|.|\" arr[1]) carry x;\\na j][y has arr[i]);\\nif (a[2 ((k limit; 7;\\nbreak;\\nif \"?\" break; a[3] a[3] \"?\" \"?\" check(string (p1 abs(d w1 {\\na[x] (a[ii] \"?\" 0;\\ndouble abs(y);\\nif str[i];\\nfor n[j]) a[3] a[3] a[i]) int_max; a[i]) 100;\\nt ((m (p) s.size();\\nstring \"?\" a[3] sieve() n);\\nif sumz carry w1 (all.size() nr 1e9) s;\\nmap<string, a[0][1] s.size();\\nstring }\\nmaxx a[3] 0;\\ndouble a[i]) sieve() int_max; max1; x;\\nysum j][y x[3]) int_min;\\nfor \"?\" (s) (a[0][2] limit; isdigit(s[i]) 1e9) a[3] a[3] -r 3;\\narr[5] b[100];\\ncin \"?\" a[0][1] \"?\" {\\na[x] nr (s) \"?\" {\\nbegin \"?\" n;\\nint 200000) x.size(); x;\\nysum (one (s) _s \"o-|ooo-o\" max(s[i], prime[num++] a[0][1] j][y s.size();\\nstring w1 {\\na[x] sieve() first, s;\\nmap<string, (s1[i rr ch;\\nfor b])]++;\\nfor (a[ii] \"?\" die_roll[6] {\\na[x] p;\\ncin x;\\na a[3] isdigit(s[i]) w1 carry ((a1 a[3] xx; (input[i] 1e9) ((a1 carry carry s1.size()) (p1 a[0][1] int_max; max(best, t--) str[i];\\nfor w1 \"?\" 1e9) carry a[0][1] }\\nsort(x, (((y ch;\\nfor -1;\\nstring 1e9)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pseudo_code_sample = \"for i = 1 to 10 print i\"\n",
        "cpp_code_sample = \"for(int i = 1; i <= 10; i++) cout << i << endl;\"\n",
        "\n",
        "pseudo_tokens = tokenizer_pseudo_to_cpp.texts_to_sequences([pseudo_code_sample])\n",
        "cpp_tokens = tokenizer_cpp.texts_to_sequences([cpp_code_sample])\n",
        "\n",
        "print(\"Pseudocode Tokens:\", pseudo_tokens)\n",
        "print(\"C++ Tokens:\", cpp_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jySKtMQdDoWw",
        "outputId": "b71166b5-ba7d-4f19-a6ff-594fbf0b5f1c"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pseudocode Tokens: [[14, 10, 2, 15, 3, 89, 9, 10]]\n",
            "C++ Tokens: [[4, 1, 28, 4, 25, 443, 13, 64, 2, 4, 2, 195]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "ybJWudtjDyPk",
        "outputId": "2b65c29c-9294-4ee7-9637-e5f90f2c5a91"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'history' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-e312ab913c3c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Training Loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Validation Loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Embedding, MultiHeadAttention, LayerNormalization, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "class Transformer(Model):\n",
        "    def __init__(self, vocab_size, seq_length, embed_dim=256, num_heads=8, ff_dim=512, **kwargs):\n",
        "        super(Transformer, self).__init__(**kwargs)\n",
        "        self.vocab_size = vocab_size\n",
        "        self.seq_length = seq_length\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.ff_dim = ff_dim\n",
        "\n",
        "        self.embedding = Embedding(vocab_size, embed_dim, mask_zero=True)\n",
        "        self.pos_encoding = Embedding(seq_length, embed_dim)\n",
        "        self.encoder = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = keras.Sequential([\n",
        "            Dense(ff_dim, activation='relu'),\n",
        "            Dense(embed_dim)\n",
        "        ])\n",
        "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
        "        self.output_layer = Dense(vocab_size, activation=\"softmax\")\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.embedding(inputs) + self.pos_encoding(tf.range(inputs.shape[1]))\n",
        "        attn_output = self.encoder(x, x, x)\n",
        "        out1 = self.layernorm1(x + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        out2 = self.layernorm2(out1 + ffn_output)\n",
        "        return self.output_layer(out2)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(Transformer, self).get_config()\n",
        "        config.update({\n",
        "            \"vocab_size\": self.vocab_size,\n",
        "            \"seq_length\": self.seq_length,\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"ff_dim\": self.ff_dim,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "        return cls(**config)\n"
      ],
      "metadata": {
        "id": "3Av7GZ7X0zA7"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Load with explicit custom_objects\n",
        "model = load_model(\"transformer_pseudo_cpp.keras\", custom_objects={\"Transformer\": Transformer})\n"
      ],
      "metadata": {
        "id": "DcAUO-SD9ZKx"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load the pseudocode-to-C++ tokenizer\n",
        "with open(\"tokenizer_pseudo_to_cpp.pkl\", \"rb\") as file:\n",
        "    tokenizer_pseudo_to_cpp = pickle.load(file)\n",
        "\n",
        "# Load the C++ tokenizer (for decoding output)\n",
        "with open(\"tokenizer_cpp.pkl\", \"rb\") as file:\n",
        "    tokenizer_cpp = pickle.load(file)\n",
        "\n"
      ],
      "metadata": {
        "id": "wwFqTAOw-J3Q"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "def generate_cpp(pseudocode, tokenizer_input, tokenizer_output, model, max_length=150):\n",
        "    \"\"\" Generates C++ code from input pseudocode using the trained Transformer model. \"\"\"\n",
        "\n",
        "    # Tokenize and pad the input pseudocode\n",
        "    input_seq = tokenizer_input.texts_to_sequences([pseudocode])\n",
        "    input_seq = pad_sequences(input_seq, maxlen=max_length, padding=\"post\")\n",
        "\n",
        "    # Predict the output sequence\n",
        "    prediction = model.predict(input_seq)\n",
        "\n",
        "    # Convert tokenized output back to text\n",
        "    predicted_tokens = np.argmax(prediction, axis=-1)[0]\n",
        "    generated_code = tokenizer_output.sequences_to_texts([predicted_tokens])[0]\n",
        "\n",
        "    return generated_code\n"
      ],
      "metadata": {
        "id": "wIGeVlPw-VHT"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pseudocode_example = \"BEGIN \\n INPUT X \\n IF X > 10 THEN PRINT 'LARGE' ELSE PRINT 'SMALL' \\n END\"\n",
        "\n",
        "generated_cpp = generate_cpp(pseudocode_example, tokenizer_pseudo_to_cpp, tokenizer_cpp, model)\n",
        "print(\"Generated C++ Code:\\n\", generated_cpp)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iDsxai60Bcml",
        "outputId": "48a180e1-89c7-408f-9563-1fea1e3565f4"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 231ms/step\n",
            "Generated C++ Code:\n",
            " current q;\n",
            "int arr[b] a[105];\n",
            "int s.size(), m;\n",
            "m--;\n",
            "for 0)\n",
            ";\n",
            "cout &in, y;\n",
            "b bfs(int cont2 l;\n",
            "ans int(m b])]++;\n",
            "for && calc(int rr has arr[i]);\n",
            "if (a[2 ((k limit; 7;\n",
            "break;\n",
            "if \"?\" break; a[3] a[3] \"?\" \"?\" check(string (p1 m; w1 {\n",
            "a[x] (a[ii] \"?\" 0;\n",
            "double abs(y);\n",
            "if str[i];\n",
            "for n[j]) a[3] a[3] a[i]) int_max; a[i]) 100;\n",
            "t ((m (p) s.size();\n",
            "string \"?\" a[3] sieve() n);\n",
            "if sumz carry w1 (all.size() nr 1e9) s;\n",
            "map<string, a[0][1] s.size();\n",
            "string }\n",
            "maxx a[3] 0;\n",
            "double a[i]) sieve() int_max; max1; x;\n",
            "ysum j][y x[3]) int_min;\n",
            "for \"?\" (s) (a[0][2] limit; isdigit(s[i]) 1e9) a[3] a[3] -r 3;\n",
            "arr[5] b[100];\n",
            "cin \"?\" a[0][1] \"?\" {\n",
            "a[x] nr (s) \"?\" {\n",
            "begin \"?\" n;\n",
            "int 200000) x.size(); x;\n",
            "ysum (one (s) _s \"o-|ooo-o\" max(s[i], prime[num++] a[0][1] j][y s.size();\n",
            "string w1 {\n",
            "a[x] sieve() first, s;\n",
            "map<string, (s1[i rr ch;\n",
            "for b])]++;\n",
            "for (a[ii] \"?\" die_roll[6] {\n",
            "a[x] p;\n",
            "cin x;\n",
            "a a[3] isdigit(s[i]) w1 carry ((a1 a[3] xx; (input[i] 1e9) ((a1 carry carry s1.size()) (p1 a[0][1] int_max; max(best, t--) str[i];\n",
            "for w1 \"?\" 1e9) carry a[0][1] }\n",
            "sort(x, (((y ch;\n",
            "for -1;\n",
            "string 1e9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Pseudocode Tokenized Example:\", tokenizer_pseudo_to_cpp.texts_to_sequences([\"INPUT X\"]))\n",
        "print(\"C++ Tokenized Example:\", tokenizer_cpp.texts_to_sequences([\"int x;\"]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hgh6dKDrBxe0",
        "outputId": "aa66f5f3-ad29-47c6-9003-59bbfb9eecae"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pseudocode Tokenized Example: [[80, 26]]\n",
            "C++ Tokenized Example: [[40, 269]]\n"
          ]
        }
      ]
    }
  ]
}